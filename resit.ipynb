{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR Resit Exercise 2024-25\n",
    "\n",
    "This notebook has been provided as a template to help you with Questions 1 and 2 of the resit exercise.  It is not compulsory to\n",
    "use it.\n",
    "\n",
    "## Template code\n",
    "\n",
    "The code below gives an example of how you might mimic \"live\" ASR on a pre-recorded audio file.  The idea is to process the file incrementally, periodically displaying the current best transcription hypothesis.  Whilst this works on a pre-recorded file for which the length of the file is known in advance, in a real application it could be used to process a streaming audio file, with the transcription displayed to the user being frequently updated.\n",
    "\n",
    "This code assumes already made a function to generate an WFST, `create_wfst()` and a decoder class, `MyViterbiDecoder`, created as part of the [labs](https://github.com/yiwang454/asr_labs) or [assignment](https://github.com/yiwang454/asr_assignment).\n",
    "\n",
    "You may wish to add new methods to the `MyViterbiDecoder` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import wer\n",
    "import observation_model\n",
    "import openfst_python as fst\n",
    "\n",
    "# ... (add your code to create WFSTs and Viterbi Decoder)\n",
    "\n",
    "f = create_wfst()\n",
    "\n",
    "# In live recognition, we want to display the current best transcription hypothesis\n",
    "# in real time.  We'll update the best transcription every UPDATE_N frames.\n",
    "# In the ASR model used in the assignment, the frame rate is 10ms (standard in ASR), \n",
    "# so UPDATE_N=10 corresponds to updating every 100ms - ie. 10 times per second.\n",
    "UPDATE_N = 10\n",
    "\n",
    "for wav_file in glob.glob('/group/teaching/asr/labs/recordings/*.wav'):    # replace path if using your own\n",
    "                                                                           # audio files    \n",
    "    decoder = MyViterbiDecoder(f, wav_file)\n",
    "\n",
    "    wav_file_length = decoder.om.observation_length()\n",
    "\n",
    "    t=1\n",
    "    while t <= wav_file_length:\n",
    "\n",
    "        # TODO: your code here to advance the Viterbi decoding\n",
    "\n",
    "        if t % UPDATE_N == 0:\n",
    "\n",
    "            # TODO: your code here to display the current best transcription hypothesis\n",
    "\n",
    "        t += 1    \n",
    "\n",
    "    # At this point, the whole wav file will have been processed, and you would normally return the final best hypothesis.\n",
    "    # This is not required for the resit exercise.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
